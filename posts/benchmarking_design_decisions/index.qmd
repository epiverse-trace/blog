---
title: "Using benchmarking to guide the adoption of dependencies in R packages"
author:
  - name: "James Mba Azam"
    orcid: "https://orcid.org/0000-0001-5782-7330"
date: last-modified
categories: [benchmarking, R, tidyverse, cli, R packages, software architecture]
format:
  html:
    toc: true
execute: 
  cache: true
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  cache = TRUE,
  message = FALSE
)
```

R package developers are constantly faced with design decisions about user
friendliness. Such design decisions often include checking that inputs are
rightly specified and how to fix or signal issues arising from intermediate
calculations or processes within a function (condition handling).

These design decisions often result in taking on dependencies. Dependencies are
external packages that enable a function to work properly. For example, the
`tidyverse` is a collection of packages that are often used together to
make data analysis easier. The `tidyverse` is a dependency for many
packages that are built on top of it.

Dependencies should not be adopted blindly without carefully considering the
overhead they introduce. Overhead here refers to the additional work that a
package must do to make the dependency work. In this article, we will
demonstrate how to use (micro)benchmarking[^1] to guide the decision to take on
dependencies.

For this demonstration, we will look at condition handling and signalling,
that is, what to do when problems arise during code execution [^2].
Condition signalling in R is classified into errors and warnings. Errors refer
to cases where something fatal happens during code execution so that it halts,
for example, when a function is run without specifying the required arguments.

```{r}
try(rnorm())
```

Warnings are used to handle less fatal issues with code execution that do not
need to halt the process.

```{r}
c(1:2) > c(1:3)
```

Base R ships with functionalities for condition handling and signalling. There is
`stop()` for signaling errors and `warning()` for throwing warnings. These base R
functions are, however, are not flexible enough for advanced formatting: colour,
progress bar, contextual pluralisation, and so forth.

Package developers often have to adopt external dependencies for feature-rich condition signalling. The [cli](https://cli.r-lib.org/articles/semantic-cli.html) package is well designed for this. `cli` offers many advanced features
that will not be discussed here, but of note is its `cli_abort()` and
`cli_warn()` functions for signalling errors and warnings respectfully.

`cli` is a great candidate for flexibility in condition signalling but one
might naturally wonder if this comes at a cost. Here,
we will investigate this through a benchmarking exercise against base R's
`warning()` function. The results of this exercise will provide us with insights
on whether its worth taking on `cli` for its features as a trade-off for its
slower speed.

## Experiment

We will consider different scenarios of number of warnings thrown to
tease out the relationship between speed of the function and number of warnings
thrown in a function with and without `cli::cli_warn()`. We will also check if
the benchmark scales with the number of warnings in the function.

Based on this description, we will need the following packages: `cli` for its warning function,
[bench](https://cran.r-project.org/web/packages/bench/index.html) for measuring
the run times, `purrr`, `tidyr`, and `dplyr` for running the functions and
wrangling the results, and and `ggplot2` for visualizing the results.

```{r}
library(bench)
library(cli)
library(ggplot2)
library(purrr)
library(dplyr)
library(tidyr)
```

Let's define a function that takes an argument `n` for the number of warnings
to throw and `pkg` for the type of package to use.

```{r}
warn_Ntimes <- function(n, pkg) {
  warning_msg <- "x must be an integer"
  switch(pkg,
    base = for (i in 1:n) {
      warning(warning_msg)
    },
    cli = for (i in 1:n) {
      cli_warn(warning_msg)
    }
  )
}
```

Let's test our function to see if it works as expected.

```{r}
warn_Ntimes(3, "base")
warn_Ntimes(3, "cli")
```

Now, we'll consider scenarios where a function throws 1, 5, 10, 15, 20, and
100 warnings using base R and `cli`.

```{r warning=FALSE}
# Number of warnings to throw
warnings <- c(1, 5, 10, 15, 20, 100)
```

Let's run benchmarks over the different scenarios and store results
in a data.frame.

```{r warning=FALSE}
bnmark_list <- warnings |>
  map(
    \(x) mark(
      "cli::cli_warn()" = warn_Ntimes(x, pkg = "cli"),
      "base::warning()" = warn_Ntimes(x, pkg = "base")
    )
  )

bnmark_tbl <- map2(
  bnmark_list,
  warnings,
  \(x, y) mutate(x, warnings = y)
) |>
  reduce(bind_rows)
```

## Results

It's time to explore the results of the data generated. We'll start by
making a boxplot of the median run times for the different scenarios.

Drum roll please...

```{r}
autoplot(bnmark_tbl, type = "boxplot") +
  labs(
    x = "Package",
    caption = "Functions throwing various numbers of warnings as indicated in the facet label"
  )
```

Finally, let's investigate the relative speeds of the two packages with
respect to the number of warnings. We will do this by dividing the speed of
`cli::cli_warn()` by the speed of `base::warning()`.

```{r}
bnmark_tbl |>
  select(expression, median, warnings) |>
  pivot_wider(
    names_from = expression,
    values_from = c(median)
    ) |> 
  mutate(
    scale = `cli::cli_warn()`/`base::warning()`
    )
```

As we can see, `cli` is consistently slower than base R but this seems not to
scale with the number of warnings thrown. Other benchmarking experiments
with `cli` have revealed linear relationships between its speed and the number
of times it invokes other functions [^3]. The developers of `cli` have also
conducted benchmarks of the `ansi_` family of functions in `cli` in comparison
to base R and the `fansi` package. They find that `cli` is consistently slower
than base R, which corroborates the results of our experiment here.
Their benchmarks are available in the cli documentation [^4]. So, should we
be worried about the speed of `cli`? Well, it depends on the context.

Most developers might argue that this is an optimisation overkill[^5]. However,
we argue that it is important to consider speed differences in context.
In the case of simple printing, the speed difference is
negligible yet disruptive and somewhat painful. However, in the grand scheme
of things, this might be nothing compared to much slower processes that
need more attention. In those cases, the developer might want to consider other
optimisation strategies such as profiling [^6]. The essence of this experiment
is to demonstrate that benchmarking is one way to help developers make design
decisions.

## Conclusion

In designing R package infrastructure with "heavy" dependencies, it might
sometimes be necessary to test your intuition about their overhead with
experiments. Here, we have demonstrated how benchmarking is one way to achieve
this. We show how a simple decision to use `cli::cli_warn()` to handle
warnings could come at the cost of a tiny loss in speed, which is worth
considering in its context.

The demonstration here can be extended to other design decisions such as
input checking, loops, object indexing, and so forth. We recommend benchmarking
as a way to help developers make design decisions. However, we also recommend
that developers consider the context of the optimisation in interpreting the results.

## Other R packages for benchmarking

* [microbenchmark](https://github.com/joshuaulrich/microbenchmark): an R
  package for comparing the execution time of R expressions.
* [rbenchmark](https://code.google.com/archive/p/rbenchmark/): an R package
  for benchmarking R code.
* [tictok](https://github.com/collectivemedia/tictoc): an R package to time
R functions
* [touchstone](https://github.com/lorenzwalthert/touchstone/tree/main): an R
  package for
  benchmarking of pull requests with statistical confidence.

[^1]: Benchmark (Wikipedia): <https://en.wikipedia.org/wiki/Benchmark_(computing)>
[^2]:  Hadley Wickham's Advanced R chapter on Exceptions and Debugging: <http://adv-r.had.co.nz/Exceptions-Debugging.html>
[^3]: Formating errors can be slow/variable: <https://github.com/r-lib/cli/issues/617>
[^4]: cli benchmarks: <https://cli.r-lib.org/articles/ansi-benchmark.html>
[^5]: Donald Knuth's quoted as having said, "The real problem is that programmers have spent far too much time worrying about efficiency in the wrong places and at the wrong times; premature optimization is the root of all evil (or at least most of it) in programming." (See <https://en.wikiquote.org/wiki/Donald_Knuth>)
[^6]: Profiling (Wikipedia): <https://csgillespie.github.io/efficientR/performance.html>
