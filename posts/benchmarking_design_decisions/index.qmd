---
title: "Using benchmarking to guide the adoption of dependencies in R packages"
author:
  - name: "James Mba Azam"
    orcid: "https://orcid.org/0000-0001-5782-7330"
date: last-modified
categories: [benchmarking, R, tidyverse, cli, R packages, software architecture]
bibliography: index.bib
format:
  html:
    toc: true
execute: 
  cache: true
---

```{r, echo = FALSE}
knitr::opts_chunk$set(
  cache = TRUE,
  message = FALSE
)
```

R package developers are constantly faced with design decisions about user
friendliness. Such design decisions often include checking that inputs are
rightly specified and returning feedback early, deciding on how to deal with
issues arising during code execution within a function, and how to
structure outputs.

These design decisions often result in taking on dependencies. Dependencies are
external packages that provide extra functionalities or features to another
package[@wickham2023r]. Quite often, the goal of taking on dependencies is to utilize its
features and reduce duplicated coding effort. For example, the
`ggplot2` package is often used, instead of R's inbuilt plotting functionalities
to make visualizations using the rich features it implements.

Dependencies should only be adopted after carefully considering their pros and
cons. In this article, we will demonstrate how to use (micro)benchmarking[^1]
to guide the decision to take on dependencies that might lead to slower
processes. In data analytics pipelines, speed is often a key consideration,
especially when working with large datasets. Therefore, it is important to
consider how the new dependency might impact the speed of an existing
process.

For this demonstration, we will look at condition signalling,
that is, how to signal to a user that an issue has arisen during code
execution [@wickham2019advanced]. Condition signalling in R is classified into errors,
warnings, and messages. Errors refer
to cases where something fatal happens during code execution so that it halts,
for example, when a function is run without specifying the required arguments.

```{r}
try(rnorm())
```

Warnings are used to handle less fatal issues with code execution that do not
need to halt the process. For example, when you compare two vectors of
different lengths, R will throw a warning but will still return a result.

```{r}
c(1:2) > c(1:3)
```

Messages are used to provide useful information about processes. For
example, sometimes, when you load a package, R will throw a message to indicate
that the package has been loaded.

```{r}
#| message: true
library(dplyr)
```

Base R ships with functionalities for condition signalling. There is
`stop()` for signaling errors, `warning()` for throwing warnings, and
`message()` for throwing messages. These base R
functions are, however, not flexible enough for advanced
formatting: colour, progress bar, contextual pluralisation, and so forth.

Package developers often have to adopt external dependencies for flexibility
in condition signalling. The [cli](https://cli.r-lib.org/articles/semantic-cli.html)
package is well designed for this. `cli` offers many advanced features
that will not be discussed here, but of note is its `cli_abort()` and
`cli_warn()` functions for signalling errors and warnings respectfully.

Code optimisation is often reserved for heavy processes like data processing steps.
However, sometimes, even small processes like printing messages to the screen
might take longer than desired. Benchmarking can be used to check whether an
introduced function from a dependency would undesirably slow down existing code.

Here, we will benchmark `cli::cli_warn()` against base R's `base::warning()` to
see if the former has any speed issues. The results of this exercise will
provide us with insights on whether its worth taking on `cli` for its features
as a trade-off for slower printing speed.

## Experiment

We will consider different scenarios of number of warnings thrown to
tease out the relationship between speed of the function and number of warnings
thrown in a function with and without `cli::cli_warn()`. We will also check if
the benchmark scales with the number of warnings in the function.

Based on this description, we will need the following packages: `cli` for its warning function,
[bench](https://cran.r-project.org/web/packages/bench/index.html) for measuring
the run times, `purrr`, `tidyr`, and `dplyr` for running the functions and
wrangling the results, and and `ggplot2` for visualizing the results.

```{r}
library(bench)
library(cli)
library(ggplot2)
library(purrr)
library(dplyr)
library(tidyr)
```

Let's define a function that takes an argument `n` for the number of warnings
to throw and `pkg` for the type of package to use.

```{r}
warn_Ntimes <- function(n, pkg) {
  warning_msg <- "x must be an integer"
  switch(pkg,
    base = for (i in seq_len(n)) {
      warning(warning_msg)
    },
    cli = for (i in seq_len(n)) {
      cli_warn(warning_msg)
    }
  )
}
```

Let's test our function to see if it works as expected.

```{r}
warn_Ntimes(3, "base")
warn_Ntimes(3, "cli")
```

Now, we'll consider scenarios where a function throws 1, 5, 10, 15, 20, and
100 warnings using base R and `cli`.

```{r warning=FALSE}
# Number of warnings to throw
warnings <- c(1, 5, 10, 15, 20, 100)
```

Let's run benchmarks over the different scenarios and store results
in a data.frame.

```{r warning=FALSE,message=FALSE}
bnmark_res <- bench::press(
  warnings = c(1, 5, 10, 15, 20, 100),
  bench::mark(
    cli = warn_Ntimes(warnings, "cli"),
    base = warn_Ntimes(warnings, "base")
    )
  )
```

## Results

It's time to explore the results of the data generated. We'll start by making
a boxplot of the median run times for the different scenarios.

Drum roll please...

```{r}
autoplot(
  bnmark_res,
  type = "boxplot"
  ) +
  labs(
    x = "Package",
    caption = "Functions throwing various numbers of warnings as indicated in the facet label"
  )
```

Finally, let's investigate the relative speeds of the two packages with
respect to the number of warnings. We will do this by dividing the median
benchmark of `cli::cli_warn()` by that of `base::warning()`.

```{r}
bnmark_relative_speeds <- bnmark_res |>
  select(expression, median, warnings) |>
  pivot_wider(
    names_from = expression,
    values_from = c(median)
    ) |> 
  mutate(
    relative_speed = cli/base
    )
bnmark_relative_speeds
```

As we can see in the new column `relative_speed`, `cli` is consistently
slower than base R and this seems not to
scale linearly with the number of warnings thrown. Other benchmarking
experiments
with `cli` have revealed linear relationships between its speed and the number
of times it invokes other functions [^3]. The developers of `cli` have also
conducted benchmarks of the `ansi_` family of functions in `cli` in comparison
to base R and the `fansi` package. They find that `cli` is consistently slower
than base R, which corroborates the results of our experiment here.
Their benchmarks are available in the cli documentation [^4]. So, should we
be worried about the speed of `cli`? Well, it depends on the context. The
"R Packages" book by Hadley Wickham and Jenny Bryan has a section on
this where they suggest approaching such a decision from a holistic, balanced,
and quantitative approach[^5]. We'll leave the reader to make their own decision
based on the their use case.

Most developers might argue that this is an optimisation overkill[^6]. However,
we argue that it is important to consider speed differences in context.
In the case of simple printing, the speed difference is
negligible yet disruptive and somewhat painful. However, in the grand scheme
of things, this might be nothing compared to much slower processes that
need more attention. In those cases, the developer might want to consider other
optimisation strategies such as profiling [^7]. The essence of this experiment
is to demonstrate that benchmarking is one way to help developers make design
decisions.

## Conclusion

In designing R package infrastructure with dependencies, it might
sometimes be necessary to test your intuition about their overhead with
experiments. Here, we have demonstrated how benchmarking is one way to achieve
this. We show how a simple decision to use `cli::cli_warn()` to handle
warnings could come at the cost of a tiny loss in speed, which is worth
considering in its context.

The demonstration here can be extended to other design decisions such as
input checking, loops, object indexing, and so forth. We recommend benchmarking
as a way to help developers make design decisions. However, we also recommend
that developers consider the context of the optimisation in interpreting the results.

## Other R packages for benchmarking

* [microbenchmark](https://github.com/joshuaulrich/microbenchmark): an R
  package for comparing the execution time of R expressions.
* [rbenchmark](https://code.google.com/archive/p/rbenchmark/): an R package
  for benchmarking R code.
* [tictok](https://github.com/collectivemedia/tictoc): an R package to time
R functions
* [touchstone](https://github.com/lorenzwalthert/touchstone/tree/main): an R
  package for
  benchmarking of pull requests with statistical confidence.

[^1]: Benchmark (Wikipedia): <https://en.wikipedia.org/wiki/Benchmark_(computing)>
[^2]: Formating errors can be slow/variable: <https://github.com/r-lib/cli/issues/617>
[^3]: cli benchmarks: <https://cli.r-lib.org/articles/ansi-benchmark.html>
[^4]: Donald Knuth's quoted as having said, "The real problem is that programmers have spent far too much time worrying about efficiency in the wrong places and at the wrong times; premature optimization is the root of all evil (or at least most of it) in programming." (See <https://en.wikiquote.org/wiki/Donald_Knuth>)
[^5]: Profiling (Wikipedia): <https://csgillespie.github.io/efficientR/performance.html>
